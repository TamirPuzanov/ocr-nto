{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d5589c-f1da-4dd8-a553-bae2839afcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/albumentations-team/albumentations.git\n",
    "!pip install -q opencv-python-headless==4.5.2.52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828f87fc-60db-4e56-81c7-bba3868ce90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unrar x /content/drive/MyDrive/train_data.rar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06260ab-dbdf-4fda-87e5-d6f7b4a14214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import cv2, os\n",
    "import torchvision\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad7b410-2a48-4b0f-9744-2fe2609ffe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(pd.read_csv(\"./train_data/labels.csv\"), test_size=0.1, random_state=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cbba13-998d-413d-aa0c-5c3da5a29bff",
   "metadata": {},
   "source": [
    "### Датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f067c4d1-234c-4f98-bfbb-e774c94200d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images, texts, enc_texts = zip(*batch)\n",
    "    images = torch.stack(images, 0)\n",
    "    text_lens = torch.LongTensor([len(text) for text in texts])\n",
    "    enc_pad_texts = pad_sequence(enc_texts, batch_first=True, padding_value=0)\n",
    "    return images, texts, enc_pad_texts, text_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34c756b-c50c-495a-a0dd-170145f802a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(torch.utils.data.Dataset):\n",
    "    def __init__(self, table, images_path, tokenizer, transforms=None, is_valid=False):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.table = table.to_numpy()\n",
    "        self.images_path = images_path\n",
    "\n",
    "        self.is_valid = is_valid\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        line = self.table[idx]\n",
    "\n",
    "        text = line[2]\n",
    "        image = self.load_image(line[1])\n",
    "\n",
    "        enc_text = self.tokenizer.encode([text])\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "        \n",
    "        return image, text, torch.LongTensor(enc_text[0])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.table.shape[0]\n",
    "    \n",
    "    def load_image(self, path):\n",
    "        image = cv2.imread(os.path.join(self.images_path, path))\n",
    "        return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a224a5f-0a76-4f3e-aa1d-bacc5fb2b875",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce23087-f11e-4270-9e9a-377f26c12e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "OOV_TOKEN = '<OOV>'\n",
    "CTC_BLANK = '<BLANK>'\n",
    "\n",
    "\n",
    "def get_char_map(alphabet):\n",
    "    char_map = {value: idx + 2 for (idx, value) in enumerate(alphabet)}\n",
    "    char_map[CTC_BLANK] = 0\n",
    "    char_map[OOV_TOKEN] = 1\n",
    "    return char_map\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, alphabet):\n",
    "        self.char_map = get_char_map(alphabet)\n",
    "        self.rev_char_map = {val: key for key, val in self.char_map.items()}\n",
    "\n",
    "    def encode(self, word_list):\n",
    "        enc_words = []\n",
    "        for word in word_list:\n",
    "            enc_words.append(\n",
    "                [self.char_map[char] if char in self.char_map\n",
    "                 else self.char_map[OOV_TOKEN]\n",
    "                 for char in word]\n",
    "            )\n",
    "        return enc_words\n",
    "\n",
    "    def get_num_chars(self):\n",
    "        return len(self.char_map)\n",
    "\n",
    "    def decode(self, enc_word_list):\n",
    "        dec_words = []\n",
    "        for word in enc_word_list:\n",
    "            word_chars = ''\n",
    "            for idx, char_enc in enumerate(word):\n",
    "                if (\n",
    "                    char_enc != self.char_map[OOV_TOKEN]\n",
    "                    and char_enc != self.char_map[CTC_BLANK]\n",
    "                    and not (idx > 0 and char_enc == word[idx - 1])\n",
    "                ):\n",
    "                    word_chars += self.rev_char_map[char_enc]\n",
    "            dec_words.append(word_chars)\n",
    "        return dec_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043d3f7f-48c6-471f-a62d-3f0107c5bfcf",
   "metadata": {},
   "source": [
    "### CRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56549aed-f6a9-4081-8905-f95aab602343",
   "metadata": {},
   "outputs": [],
   "source": [
    "from resnet import resnet18, resnet34, resnet50\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, hidden_size, num_layers,\n",
    "            dropout=dropout, bidirectional=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.lstm.flatten_parameters()\n",
    "        return self.lstm(x)[0]\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, number_class_symbols, fe_d_out=512, rnn_size=2):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = [\n",
    "            BiLSTM(fe_d_out, number_class_symbols),\n",
    "            nn.Linear(number_class_symbols * 2, number_class_symbols)\n",
    "        ]\n",
    "\n",
    "        for _ in range(rnn_size - 1):\n",
    "            layers += [BiLSTM(number_class_symbols, number_class_symbols),\n",
    "                      nn.Linear(number_class_symbols * 2, number_class_symbols)]\n",
    "\n",
    "        self.feature_extractor = resnet18(pretrained=False)\n",
    "        self.rnn = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        n, c, h, w = x.shape\n",
    "\n",
    "        x = x.permute(0, 1, 3, 2).reshape((n, c, w * h))\n",
    "        return F.log_softmax(self.rnn(x.permute(2, 0, 1)), dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae073b8-9c7f-4236-bebd-9a6a35d8b86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y_true, y_pred):\n",
    "    scores = []\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        scores.append(true == pred)\n",
    "    avg_score = np.mean(scores)\n",
    "    return avg_score\n",
    "\n",
    "def val_loop(data_loader, model, tokenizer, device):\n",
    "    acc_avg = AverageMeter()\n",
    "    for images, texts, _, _ in tqdm(data_loader, total=len(data_loader)):\n",
    "        batch_size = len(texts)\n",
    "        text_preds = predict(images, model, tokenizer, device)\n",
    "        acc_avg.update(get_accuracy(texts, text_preds), batch_size)\n",
    "    print(f'Validation, acc: {acc_avg.avg:.4f}')\n",
    "    return acc_avg.avg\n",
    "\n",
    "\n",
    "def train_loop(data_loader, model, criterion, optimizer, epoch):\n",
    "    loss_avg = AverageMeter()\n",
    "    model.train()\n",
    "\n",
    "    tq = tqdm(data_loader, total=len(data_loader), desc=f\"Epoch #{epoch}\")\n",
    "\n",
    "    for images, texts, enc_pad_texts, text_lens in tq:\n",
    "        model.zero_grad()\n",
    "        images = images.to(DEVICE)\n",
    "        batch_size = len(texts)\n",
    "        output = model(images)\n",
    "        output_lenghts = torch.full(\n",
    "            size=(output.size(1),),\n",
    "            fill_value=output.size(0),\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        loss = criterion(output, enc_pad_texts, output_lenghts, text_lens)\n",
    "        item = loss.item()\n",
    "\n",
    "        tq.set_postfix({\n",
    "            \"loss\": item,\n",
    "        })\n",
    "\n",
    "        loss_avg.update(item, batch_size)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 2)\n",
    "        optimizer.step()\n",
    "    for param_group in optimizer.param_groups:\n",
    "        lr = param_group['lr']\n",
    "    print(f'\\nEpoch {epoch}, Loss: {loss_avg.avg:.5f}, LR: {lr:.7f}')\n",
    "    return loss_avg.avg\n",
    "\n",
    "\n",
    "def predict(images, model, tokenizer, device):\n",
    "    model.eval()\n",
    "    images = images.to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(images)\n",
    "    pred = torch.argmax(output.detach().cpu(), -1).permute(1, 0).numpy()\n",
    "    text_preds = tokenizer.decode(pred)\n",
    "    return text_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f021eb0b-2ffa-4552-bd08-764a07cf417d",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d24ae96-aec9-4a1d-bcd4-26dea58174c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb62b9d-8600-4936-8b64-108e355753b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations.augmentations.transforms import ToGray\n",
    "from albumentations.augmentations.geometric.rotate import Rotate\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Rotate(4),\n",
    "    A.Resize(128, 356),\n",
    "    A.ChannelShuffle(p=0.2),\n",
    "    A.ColorJitter(p=1),\n",
    "    A.RandomShadow(p=0.3),\n",
    "    A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), p=0.25, always_apply=False),\n",
    "    A.JpegCompression(quality_lower=75, p=0.5),\n",
    "    A.ToGray(p=0.3),\n",
    "\n",
    "    A.Normalize(p=1),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "def train_transform_fn(x):\n",
    "    return train_transform(image=x)[\"image\"]\n",
    "\n",
    "valid_transform = A.Compose([\n",
    "    A.Resize(128, 356),\n",
    "    A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), p=0.25, always_apply=False),\n",
    "    A.JpegCompression(quality_lower=75, p=0.5),\n",
    "\n",
    "    A.Normalize(p=1),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "def valid_transform_fn(x):\n",
    "    return valid_transform(image=x)[\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9284afc6-53f8-4138-901e-fb5a85757545",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = \"\"\" !\"%\\'()*+,-./0123456789:;<=>?ABCDEFGHIJKLMNOPRSTUVWXY[]_abcdefghijklmnopqrstuvwxyz|}ЁАБВГДЕЖЗИКЛМНОПРСТУФХЦЧШЩЫЬЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяё№\"\"\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = Tokenizer(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4e7989-3211-4873-87b6-9f759393da35",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CRNN(tokenizer.get_num_chars(), 512, rnn_size=2)\n",
    "model.load_state_dict(torch.load(\"checkpoints/best.pth\"))\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "criterion = torch.nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001,\n",
    "                              weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer=optimizer, mode='max', factor=0.5, patience=15)\n",
    "best_acc = -np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc80b3b-60f7-4985-aa3c-14502a6fddae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1000):\n",
    "    loss = train_loop(train_loader, model, criterion, optimizer, epoch)\n",
    "    acc = val_loop(valid_loader, model, tokenizer, DEVICE)\n",
    "\n",
    "    if acc > best_acc:\n",
    "        torch.save(model.state_dict(), \"checkpoints/best.pth\")\n",
    "        print(\"model saved!\")\n",
    "        best_acc = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6105311-5ad5-4b59-ae1e-fdba4280410f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loop(valid_loader, model, tokenizer, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d377b12-9cb4-4069-9751-60f1ded869ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"checkpoints/final.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
